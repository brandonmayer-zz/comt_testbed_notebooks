{
 "metadata": {
  "name": "",
  "signature": "sha256:1fca44729ecf725ba41d350ebf81fee80f2a836d27a2e0de9bf0edab649387d2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from owslib import fes, csw\n",
      "from netCDF4 import Dataset, num2date, date2num\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "import json\n",
      "import re"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "endpoint = \"http://www.ngdc.noaa.gov/geoportal/csw\"\n",
      "uuid = '8BF00750-66C7-49FF-8894-4D4F96FD86C0'\n",
      "uuid_filter = fes.PropertyIsEqualTo(propertyname='sys.siteuuid', literal=\"{{{0}}}\".format(uuid))\n",
      "timeout = 120\n",
      "csw_catalogue = csw.CatalogueServiceWeb(endpoint, timeout=timeout)\n",
      "csw_catalogue.getrecords2([uuid_filter], esn='full', maxrecords=999999)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "urls = {}\n",
      "for name, record in csw_catalogue.records.iteritems():\n",
      "    legal_name = re.sub('[ .!,;\\-/\\\\\\\\]','_', name)\n",
      "    for ref in record.references:\n",
      "        if 'odp' in ref.get('scheme').split(\":\"):\n",
      "            urls[legal_name] = ref['url']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_temporal_extent(ncdataset):\n",
      "    temporal_extent = []\n",
      "    \n",
      "    timeobj = ncdataset.variables.get('time')\n",
      "    if timeobj:\n",
      "        tkwargs = {}\n",
      "        if hasattr(timeobj, 'units'):\n",
      "            tkwargs['units'] = timeobj.units\n",
      "        if hasattr(timeobj, 'calendar'):\n",
      "            tkwargs['calendar'] = timeobj.calendar.lower()\n",
      "            \n",
      "        times = timeobj[:]\n",
      "        tmin = np.nanmin(times)\n",
      "        tmax = np.nanmax(times)\n",
      "        dates = []\n",
      "        for t in times:\n",
      "            try:\n",
      "                date = num2date(t, **kwargs)\n",
      "                print date\n",
      "                dates.append(date)\n",
      "            except:\n",
      "                pass\n",
      "        if len(dates) >= 2:\n",
      "            temporal_extent = [dates[0], dates[-1]]\n",
      "    return temporal_extent"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "missing_time = []\n",
      "temporal_extent = {}\n",
      "# for i, (name, url) in enumerate(urls.iteritems()):\n",
      "for i, (name, url) in enumerate(zip(urls.keys()[:5],urls.values()[:5])):\n",
      "    print ('\\rProcessing {0} ({1} of {2})'.format(name, i+1, len(urls))),\n",
      "    temporal_extent[name] = []\n",
      "    d = Dataset(url,'r')\n",
      "    if not 'time' in d.variables:\n",
      "        missing_time.append(name)\n",
      "    else:\n",
      "        temporal_extent[name] = get_temporal_extent(d)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "Processing inundation_tropical_MDL_SLOSH_Hurricane_Rita_final_run_egm3 (1 of 111) "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "Processing inundation_tropical_UND_ADCIRC_Hurricane_Ike_3D_final_run_without_waves (2 of 111) "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "Processing inundation_tropical_USF_FVCOM_Tides_only_2D_final_run (3 of 111) "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "Processing inundation_tropical_UND_SLOSH_Hurricane_Ike_final_run_egm3 (4 of 111) "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "Processing shelf_hypoxia_NOAA_NGOM_2005_2011_NGOM (5 of 111)\n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print temporal_extent"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'inundation_tropical_MDL_SLOSH_Hurricane_Rita_final_run_egm3': [], 'inundation_tropical_USF_FVCOM_Tides_only_2D_final_run': [], 'inundation_tropical_UND_SLOSH_Hurricane_Ike_final_run_egm3': [], 'shelf_hypoxia_NOAA_NGOM_2005_2011_NGOM': [], 'inundation_tropical_UND_ADCIRC_Hurricane_Ike_3D_final_run_without_waves': []}\n"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This particular datset has units of hours and the final (approx) 25 elements of the time offset array have value 9e36, causing the num2date to fail."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "badurl = urls['estuarine_hypoxia_VIMS_EFDC_2004_DO3d']\n",
      "print badurl\n",
      "d = Dataset(badurl,'r')\n",
      "d.variables.keys()\n",
      "times = d.variables['time'][:]\n",
      "print d.variables['time'].units\n",
      "print times\n",
      "print times[0]\n",
      "print times[-1]\n",
      "np.where(times==times[-1])\n",
      "print times[8627:]\n",
      "print len(times)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "http://comt.sura.org/thredds/dodsC/data/comt_1_archive/estuarine_hypoxia/VIMS_EFDC/2004_DO3d\n",
        "hours since 2004-01-01 00:00:00"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[  5.00000000e-01   1.50000000e+00   2.50000000e+00 ...,   9.96920997e+36\n",
        "   9.96920997e+36   9.96920997e+36]\n",
        "0.5\n",
        "9.96920996839e+36\n",
        "[  8.62750000e+03   9.96920997e+36   9.96920997e+36   9.96920997e+36\n",
        "   9.96920997e+36   9.96920997e+36   9.96920997e+36   9.96920997e+36\n",
        "   9.96920997e+36   9.96920997e+36   9.96920997e+36   9.96920997e+36\n",
        "   9.96920997e+36   9.96920997e+36   9.96920997e+36   9.96920997e+36\n",
        "   9.96920997e+36   9.96920997e+36   9.96920997e+36   9.96920997e+36\n",
        "   9.96920997e+36   9.96920997e+36   9.96920997e+36   9.96920997e+36]\n",
        "8651\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d = Dataset(badurl,'r')\n",
      "times = d.variables['time'][:]\n",
      "tkwargs = {}\n",
      "tkwargs['units'] = d.variables['time'].units\n",
      "if hasattr(d.variables['time'],'calendar')\n",
      "    tkwargs['calendar'] = d.variables['calendar'].calendar\n",
      "print tkwargs\n",
      "dates = []\n",
      "for t in times:\n",
      "    print t"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "KeyError",
       "evalue": "'calendar'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-50-b1e682b18b3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'units'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'calendar'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'calendar'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalendar\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mtkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mdates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mKeyError\u001b[0m: 'calendar'"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print d.variables['time'].__dict__"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "OrderedDict([(u'units', u'hours since 2004-01-01 00:00:00'), (u'long_name', u'time'), (u'standard_name', u'time'), (u'_ChunkSize', 10)])\n"
       ]
      }
     ],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}